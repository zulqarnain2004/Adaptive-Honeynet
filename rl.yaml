# Reinforcement Learning Configuration

# Agent Parameters
agent:
  type: q_learning  # q_learning, dqn, ddpg
  learning_rate: 0.001  # alpha
  gamma: 0.99  # discount factor
  epsilon: 0.1  # exploration rate
  epsilon_decay: 0.995
  epsilon_min: 0.01
  memory_size: 10000
  batch_size: 32
  update_target_every: 100

# Environment Parameters
environment:
  type: network_simulator
  nodes: 20
  max_honeypots: 8
  min_honeypots: 4
  observation_space: 10  # State dimensions
  action_space: 4  # Number of possible actions
  
  # State components
  state_features:
    - active_honeypots
    - cpu_usage
    - memory_usage
    - attack_intensity
    - false_positive_rate
    - detection_accuracy
    - network_load
    - compromised_nodes
    - resource_utilization
    - threat_level

# Action Space
actions:
  0: deploy_honeypot
  1: migrate_honeypot
  2: reconfigure_service
  3: do_nothing

# Reward Function
rewards:
  attack_detected: 1.0
  false_detection: -1.0
  wasted_resource: -0.5
  efficient_placement: 0.2
  successful_migration: 0.3
  constraint_violation: -0.8
  system_stability: 0.1

# Training Parameters
training:
  episodes: 1000
  max_steps: 100
  log_interval: 10
  save_interval: 100
  render_interval: 50
  
  # Early stopping
  early_stopping:
    enabled: true
    patience: 50
    min_delta: 0.001
    
  # Checkpointing
  checkpoint:
    enabled: true
    directory: models/rl_checkpoints
    save_best: true

# Q-Learning Specific
q_learning:
  initialization: zeros  # zeros, random, optimistic
  update_method: sarsa  # sarsa, expected_sarsa
  eligibility_traces: false
  lambda: 0.9

# DQN Specific (if using)
dqn:
  hidden_layers: [64, 64]
  activation: relu
  optimizer: adam
  loss: mse
  target_update: soft  # soft, hard
  tau: 0.001

# Performance Targets
targets:
  average_reward: 0.45  # From screenshot
  convergence_episodes: 500
  stability_threshold: 0.01
  exploration_rate: 0.1