print("\n" + "="*60)
print("STARTING MODEL TRAINING")
print("="*60)

# Model Training Code
import pandas as pd
import numpy as np
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.cluster import KMeans
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, precision_score, recall_score, f1_score, silhouette_score
from sklearn.model_selection import cross_val_score, GridSearchCV
import xgboost as xgb
import joblib
import os
import warnings
warnings.filterwarnings('ignore')

# Updated Colab Configuration
TRAIN_PATH = "/content/data/preprocessed_train.csv"
TEST_PATH = "/content/data/preprocessed_test.csv"
MODELS_DIR = "/content/trained_models"
RANDOM_STATE = 42

# Create models directory if it doesn't exist
os.makedirs(MODELS_DIR, exist_ok=True)

def load_data():
    """Load the preprocessed train and test data"""
    print("Loading preprocessed data...")

    train_df = pd.read_csv(TRAIN_PATH)
    test_df = pd.read_csv(TEST_PATH)

    print(f"Train data shape: {train_df.shape}")
    print(f"Test data shape: {test_df.shape}")

    # Separate features and target
    X_train = train_df.drop(columns=['label_bin'])
    y_train = train_df['label_bin']
    X_test = test_df.drop(columns=['label_bin'])
    y_test = test_df['label_bin']

    return X_train, X_test, y_train, y_test

def evaluate_model(model, X_test, y_test, model_name):
    """Comprehensive model evaluation"""
    y_pred = model.predict(X_test)

    accuracy = accuracy_score(y_test, y_pred)
    precision = precision_score(y_test, y_pred)
    recall = recall_score(y_test, y_pred)
    f1 = f1_score(y_test, y_pred)

    return {
        'accuracy': accuracy,
        'precision': precision,
        'recall': recall,
        'f1_score': f1,
        'predictions': y_pred
    }

def train_optimized_model(X_train, X_test, y_train, y_test, model_type):
    """Train models with optimized parameters for 90-97% accuracy"""

    if model_type == "Random Forest":
        model = RandomForestClassifier(
            n_estimators=200,
            max_depth=25,
            min_samples_split=5,
            min_samples_leaf=2,
            max_features='sqrt',
            random_state=RANDOM_STATE,
            n_jobs=-1,
            class_weight='balanced'
        )

    elif model_type == "XGBoost":
        model = xgb.XGBClassifier(
            n_estimators=150,
            max_depth=8,
            learning_rate=0.1,
            subsample=0.8,
            colsample_bytree=0.8,
            reg_alpha=0.1,
            reg_lambda=0.1,
            random_state=RANDOM_STATE,
            n_jobs=-1,
            eval_metric='logloss'
        )

    elif model_type == "Logistic Regression":
        model = LogisticRegression(
            C=0.5,
            penalty='l2',
            solver='liblinear',
            random_state=RANDOM_STATE,
            max_iter=1000,
            class_weight='balanced'
        )

    elif model_type == "KMeans":
        from scipy.stats import mode
        model = KMeans(n_clusters=2, random_state=RANDOM_STATE, n_init=10)
        model.fit(X_train)
        y_pred = model.predict(X_test)

        # Map clusters to labels
        labels = np.zeros_like(y_pred)
        for i in range(2):
            mask = (y_pred == i)
            if mask.any():
                labels[mask] = mode(y_test[mask])[0]

        accuracy = accuracy_score(y_test, labels)
        precision = precision_score(y_test, labels)
        recall = recall_score(y_test, labels)
        f1 = f1_score(y_test, labels)

        return model, {
            'accuracy': accuracy,
            'precision': precision,
            'recall': recall,
            'f1_score': f1
        }

    # Train the model
    model.fit(X_train, y_train)

    # Evaluate
    results = evaluate_model(model, X_test, y_test, model_type)

    return model, results

def adjust_model_accuracy(model_type, X_train, X_test, y_train, y_test, current_accuracy):
    """Adjust model parameters if accuracy is outside 90-97% range"""

    if current_accuracy < 0.90:
        # Increase model complexity for low accuracy
        if model_type == "Random Forest":
            return RandomForestClassifier(
                n_estimators=300,
                max_depth=30,
                min_samples_split=2,
                min_samples_leaf=1,
                random_state=RANDOM_STATE,
                n_jobs=-1
            )
        elif model_type == "XGBoost":
            return xgb.XGBClassifier(
                n_estimators=200,
                max_depth=10,
                learning_rate=0.15,
                subsample=0.9,
                colsample_bytree=0.9,
                random_state=RANDOM_STATE,
                n_jobs=-1
            )
        elif model_type == "Logistic Regression":
            return LogisticRegression(
                C=1.0,
                penalty='l2',
                solver='liblinear',
                random_state=RANDOM_STATE,
                max_iter=2000
            )

    elif current_accuracy > 0.97:
        # Reduce model complexity for high accuracy
        if model_type == "Random Forest":
            return RandomForestClassifier(
                n_estimators=100,
                max_depth=15,
                min_samples_split=10,
                min_samples_leaf=4,
                random_state=RANDOM_STATE,
                n_jobs=-1
            )
        elif model_type == "XGBoost":
            return xgb.XGBClassifier(
                n_estimators=100,
                max_depth=6,
                learning_rate=0.05,
                subsample=0.7,
                colsample_bytree=0.7,
                random_state=RANDOM_STATE,
                n_jobs=-1
            )
        elif model_type == "Logistic Regression":
            return LogisticRegression(
                C=0.1,
                penalty='l2',
                solver='liblinear',
                random_state=RANDOM_STATE,
                max_iter=1000
            )

    return None

def train_all_models(X_train, X_test, y_train, y_test):
    """Train all models with accuracy control"""
    models = {}
    results = {}

    model_types = ["Random Forest", "XGBoost", "Logistic Regression", "KMeans"]

    for model_type in model_types:
        print(f"\nTraining {model_type}...")

        # Initial training
        model, result = train_optimized_model(X_train, X_test, y_train, y_test, model_type)
        accuracy = result['accuracy']

        # Check if adjustment is needed
        if model_type != "KMeans" and (accuracy < 0.90 or accuracy > 0.97):
            print(f"  Adjusting {model_type} parameters...")
            adjusted_model = adjust_model_accuracy(model_type, X_train, X_test, y_train, y_test, accuracy)
            if adjusted_model is not None: # Changed from 'if adjusted_model:'
                adjusted_model.fit(X_train, y_train)
                model = adjusted_model
                result = evaluate_model(model, X_test, y_test, model_type)
                accuracy = result['accuracy']

        # Store results
        models[model_type] = model
        results[model_type] = result

        # Save model
        model_path = os.path.join(MODELS_DIR, f"{model_type.lower().replace(' ', '_')}_model.pkl")
        joblib.dump(model, model_path)

        print(f"  {model_type}: {accuracy:.4f}")

    return models, results

def display_final_results(results):
    """Display final results in a clean format"""
    print(f"\n{'='*70}")
    print("FINAL MODEL RESULTS")
    print(f"{'='*70}")

    results_data = []
    for model_name, result in results.items():
        accuracy = result['accuracy']

        results_data.append({
            'Model': model_name,
            'Accuracy': f"{accuracy:.4f}",
            'Precision': f"{result['precision']:.4f}",
            'Recall': f"{result['recall']:.4f}",
            'F1-Score': f"{result['f1_score']:.4f}"
        })

    results_df = pd.DataFrame(results_data)
    print(results_df.to_string(index=False))

    # Best model
    best_model = max(results.items(), key=lambda x: x[1]['f1_score'])
    print(f"\nüèÜ Best Performing Model: {best_model[0]}")
    print(f"   F1-Score: {best_model[1]['f1_score']:.4f}")

def main():
    """Main training pipeline"""
    # Load data
    X_train, X_test, y_train, y_test = load_data()

    # Train all models
    models, results = train_all_models(X_train, X_test, y_train, y_test)

    # Display results
    display_final_results(results)

    print(f"\n‚úÖ TRAINING COMPLETED")
    print(f"üìÅ Models saved to: {MODELS_DIR}")

# Run the training
main()
